---
title: 'HarvardX:PH125.9x Data Science - Capstone CYO: Customer Segmentation Project'
author: "Richard Jonyo"
date: "04 March 2022"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Executive Summary

The goal of this project was to identify the segments of customers  based on common characteristics or patterns. Segmentation of customer can take many forms, based on demographic, geographic, interest, behavior or a combination of these characteristics. Through analysing customer purchases and product sales history, we can group customers into groups that behave similarly, and use the insights to drive decision making especially targeted marketing strategies.

For this project we shall employ k-mean Clustering which is the essential algorithm for clustering unlabelled dataset. We shall have to format the data in a way the algorithm can process, and we shall let it determine the customer segments.

This project is part of the HarvardX:PH125.9x Data Science: Capstone course and we use E-commerce dataset from [E-Commerce Dataset](https://www.kaggle.com/fabiendaniel/customer-segmentation/data) from Kaggel.com repository. The E-commerce database lists purchases made by over 4000 customers over a period of one year (from 2010/12/01 to 2011/12/09).

Exploratory analysis was conducted on the data using R and R Studio, a language and a software environment for statistical computing. R Markdown, a simple formatting syntax for authoring HTML, PDF, and MS Word documents and a component of R Studio was used to compile the report.

# 2. Methods and Exploratory Analysis

This section helps us to understand the structure of the E-Commerce Dataset for us to gain insights that will aid in a better analysis for customer segmentation. It explains the process and techniques used, including data cleaning, exploration, visualization, insights gained, and the segmentation approach used.

### 2.1 Required Libraries

The project utilized and loaded several CRAN libraries to assist with the analysis. The libraries were automatically downloaded and installed during code execution. These included: ggplot2, dplyr, tidyr, DataExplorer, lubridate, heatmaply, dlookr, and highcharter libraries.


```{r Loading Packages, message = FALSE, warning = FALSE}
# Note: this process could take a couple of minutes
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(dataexplorer)) install.packages("dataexplorer", repos = "http://cran.us.r-project.org")
if(!require(heatmaply)) install.packages("heatmaply", repos = "http://cran.us.r-project.org")
if(!require(dlookr)) install.packages("dlookr", repos = "http://cran.us.r-project.org")
if(!require(highcharter)) install.packages("highcharter", repos = "http://cran.us.r-project.org")
if(!require(purrr)) install.packages("purrr", repos = "http://cran.us.r-project.org")
if(!require(factoextra)) install.packages("factoextra", repos = "http://cran.us.r-project.org")

library(ggplot2)
library(dplyr)
library(tidyr)
library(DataExplorer)
library(lubridate)
library(heatmaply)
library(dlookr)
library(highcharter)
library(purrr)
library(factoextra)
```

### 2.2 The E-Commerce Dataset

The E-Commerce Dataset contains 541,909 transactions by 4,373 unique customers. Each customer is represented by a CustomerID and customers can have multiple transactions.

```{r Loading Dataset, message = FALSE, warning = FALSE}
# the E-Commerce Dataset:
# https://www.kaggle.com/fabiendaniel/customer-segmentation/data

#dataset downloaded and loaded locally
customer_data <- read.csv("dataset/data.csv")
```


```{r Dataset Review, message = FALSE, warning = FALSE}
head(customer_data, 5) #Preview customer_data
```

*The following is a brief description of the data.

+ **InvoiceNo:** A 6-digit number uniquely assigned to each transaction. If the code starts with the letter C, it indicates a cancellation.
+ **StockCode:** A 5-digit number that is uniquely assigned to each product.
+ **Description:** Product name.
+ **Quantity:** The quantities of each product per transaction.
+ **InvoiceDate:** Invoice date and time. Includes the day and time when a transaction was generated.
+ **UnitPrice:** Product price per unit.
+ **CustomerID:** A 5-digit number uniquely assigned to each customer.
+ **Country:** The name of the customer's country.
```

The dataset has 541,909 observations and 8 columns.

```{r Dataset Characteristics, message = FALSE, warning = FALSE}
dim(customer_data) #541,909 observations and 8 columns
```

We see three data types: character, integer, and number.

```{r Data Types, message = FALSE, warning = FALSE}
str(customer_data) #view datatypes
```

The dataset has 8 columns namely: InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, and Country.

```{r Column Names, message = FALSE, warning = FALSE}
names(customer_data)#view column names
```

The dataset has 4,373 unique customers.

```{r Unique Customers, message = FALSE, warning = FALSE}
length(unique(customer_data$CustomerID))#4,373 unique customers
```

#### Missing Values

There are some missing values in the dataset on the CustomerID column.

```{r NA, message = FALSE, warning = FALSE}
anyNA(customer_data) #check missing values - results to TRUE
```

We plot missing values and discover that customeID has many missing values.

```{r Missing Values, message = FALSE, warning = FALSE}
#We plot missing values
plot_missing(customer_data)
```

The Quantity and UnitPrice seem to be the most important in the dataset. We notice that both varibales have negative values. We check their summaries as below:

```{r Quantity and UnitPrice, message = FALSE, warning = FALSE}
summary(customer_data$Quantity)#Quantity summary
summary(customer_data$UnitPrice)#UnitPrice summary
```

We notice that we have negative Quantities within the dataset which reflect cancelled orders. These are indicated with the C letter in front of the Invoice Number.

```{r Negative Values, message = FALSE, warning = FALSE}
#Checking Quantities with negative values
quantityCheck <- customer_data %>% 
  filter(Quantity < 0) %>% 
  arrange(Quantity)
head(quantityCheck, 5)
```

#### Negative values

We replace all negative quantities with NA so that they don't not affect our results. In addition, we delete all transactions that do not have a customerID and remain with 397,884 observations.

```{r Replace Negative Values, message = FALSE, warning = FALSE}
#Replace all negative Quantity and Price with NA
#Delete any customerID with NA
customer_data <- customer_data %>% 
  mutate(Quantity = replace(Quantity, Quantity<=0, NA),
         UnitPrice = replace(UnitPrice, UnitPrice<=0, NA))
customer_data <- customer_data %>%
  drop_na()
dim(customer_data) #we remain with 397,884 observations
```


#### Outliers

We check for the presence of outliers. We notice that most of the products that are being sold are mostly a low priced. Some of these outliers could be cancelled or wrong orders that got returned and thus were assigned with a negative value. We leave the outliers as removing them would have undermined the analysis.

A charts for the outliers (using the *dlookr package*) reveals that there are some negative values in our data for Quantity, as well as some zero value inputs.

```{r Plot - Quantity Outliers, message = FALSE, warning = FALSE}
#We check again the presence of outliers
plot_outlier(customer_data, Quantity, col = "#FF3399")
```


```{r Plot - UnitPrice Outliers, message = FALSE, warning = FALSE}
plot_outlier(customer_data, UnitPrice, col = "#FF3399")
```

#### Create Date variables

We separate date and time components of invoice date.

```{r cleaning Dates, message = FALSE, warning = FALSE}
#We separate date and time components of invoice date
customer_data$date <- sapply(customer_data$InvoiceDate, FUN = function(x) {strsplit(x, split = '[ ]')[[1]][1]})
customer_data$time <- sapply(customer_data$InvoiceDate, FUN = function(x) {strsplit(x, split = '[ ]')[[1]][2]})
```

We create month, year and hour of day variables.

```{r Date Details, message = FALSE, warning = FALSE}
#we create month, year and hour of day columns
customer_data$month <- sapply(customer_data$date, FUN = function(x) {strsplit(x, split = '[/]')[[1]][1]})
customer_data$year <- sapply(customer_data$date, FUN = function(x) {strsplit(x, split = '[/]')[[1]][3]})
customer_data$hourOfDay <- sapply(customer_data$time, FUN = function(x) {strsplit(x, split = '[:]')[[1]][1]})
tmp <- customer_data %>% select(CustomerID, Country, date, time, month, year, hourOfDay)
head(tmp)
```

We convert date column to date format and create a new column for day of the week, using the *wday function* from the lubridate package.

```{r Date Format, message = FALSE, warning = FALSE}
#We convert date column to date format
#We can create day of the week column
customer_data$date <- as.Date(customer_data$date, "%m/%d/%Y")
str(customer_data)
customer_data$dayOfWeek <- wday(customer_data$date, label=TRUE)
tmp <- customer_data %>% select(CustomerID, Country, date, time, month, year, hourOfDay)
head(tmp)
```

#### Create a TotalCost Column

We add TotalCost column by calculating Quantity x UnitPrice.

```{r TotalCost Column, message = FALSE, warning = FALSE}
#add TotalCost column
customer_data <- customer_data %>% mutate(TotalCost = Quantity * UnitPrice)
```

We convert appropriate columns (month, year, hourOfDay, dayOfWeek, and Country) into factors.

```{r Convert Columns, message = FALSE, warning = FALSE}
#we turn the appropriate variables into factors
customer_data$month <- as.factor(customer_data$month)
customer_data$year <- as.factor(customer_data$year)
levels(customer_data$year) <- c(2010,2011)
customer_data$hourOfDay <- as.factor(customer_data$hourOfDay)
customer_data$dayOfWeek <- as.factor(customer_data$dayOfWeek)
customer_data$Country <- as.factor(customer_data$Country)
str(customer_data)
```

### 2.4 Exploratory Analysis

We have a good dataset to start performing some summary analyses. We employ Exploratory Data Analysis (EDA) to conduct an initial investigation inside the dataset and observe common patterns, spot anomalies and retrieve useful information about the data in a graphical way. We need to understand the dataset before starting to develop the models hence an exploratory analysis is significant.

Below is a quick summary of the dataset:

```{r Summary of edx, message = FALSE, warning = FALSE}
summary(customer_data)           
```

The dataset has `r length(unique(customer_data$CustomerID))` unique customerIDs and `r length(unique(customer_data$InvoiceNo))` unique invoice numbers.

```{r Unique Values, message = FALSE, warning = FALSE}
length(unique(customer_data$CustomerID)) #4,373 unique customerIDs
length(unique(customer_data$InvoiceNo)) #18,532 unique invoice no.s
```


### 2.4.1 Data cleaning

We plot the missing values using the *DataExplorer package*. Looking at the size of the dataset and the missing value plot, it seems we have missing values which we can remove and still have a good-sized set of data to work on.

```{r, echo=FALSE}
#remove missing values
customer_data <- subset(customer_data, !is.na(customer_data$CustomerID))
dim(customer_data) #We remain with 406,829 observations without NAs
```
We remain with `r dim(customer_data)` observations without NAs.

### Revenue Summaries

From the chart below there seem to be a steady positive increase in revenue over time with the highest peak in September 2011.

```{r Revenue Summary, message = FALSE, warning = FALSE}
#Plot revenues over time
options(repr.plot.width=8, repr.plot.height=3)
customer_data %>%
  group_by(date) %>%
  summarise(revenue = sum(TotalCost)) %>%
  ggplot(aes(x = date, y = revenue)) + 
  geom_line() + 
  geom_smooth(method = 'auto', se = FALSE) + 
  labs(x = 'Date', y = 'Revenue (£)', title = 'Revenue by Date')
```

The chart below shows the Revenue by Day of Week.

```{r Revenue Per Day of Week, message = FALSE, warning = FALSE}
# Plot Revenue by Day of Week
customer_data %>%
  group_by(dayOfWeek) %>%
  summarise(revenue = sum(TotalCost)) %>%
  ggplot(aes(x = dayOfWeek, y = revenue)) + 
  geom_col() + 
  labs(x = 'Day of Week', y = 'Revenue (£)', title = 'Revenue by Day of Week')
```
  
The chart below shows a summary of revenue that is generated on each particular weekday.It is evident that more revenue is generated on Thursdays and the least revenue is generated on Sundays.
  
```{r Summary Revenue per day, message = FALSE, warning = FALSE}
#Summary of revenue generated on each weekday
weekdaySummary <- customer_data %>%
  group_by(date, dayOfWeek) %>%
  summarise(revenue = sum(TotalCost), transactions = n_distinct(InvoiceNo)) %>%
  mutate(aveOrdVal = (round((revenue / transactions),2))) %>%
  ungroup()
```

The plot below shows the Revenue by Day of the Week.

```{r Revenue per day, message = FALSE, warning = FALSE}
#Plot of Revenue by Day of the Week
ggplot(weekdaySummary, aes(x = dayOfWeek, y = revenue)) + 
  geom_boxplot() + 
  labs(x = 'Day of the Week', y = 'Revenue', title = 'Revenue by Day of the Week')
```

The chart below shows the transactions by day of the Week.It is evident that there are more transactions on Thursdays which is also when we have the most revenue generated.

```{r Transactions per day, message = FALSE, warning = FALSE}
#Plot of Transactions by Day of the Week
ggplot(weekdaySummary, aes(x = dayOfWeek, y = transactions)) + 
  geom_boxplot() + labs(x = 'Day of the Week', y = 'Daily Transactions', title = 'Transactions by Day of the Week')
```

The differences in the amount of revenue on each day of the week is driven by a difference in the no. of transactions, rather than the average order value. 
```{r Average Order Value per day, message = FALSE, warning = FALSE}
#Plot of Average Order Value by Day of the Week
ggplot(weekdaySummary, aes(x = dayOfWeek, y = aveOrdVal)) + 
  geom_boxplot() + labs(x = 'Day of the Week', y = 'Average Order Value', title = 'Average Order Value by Day of the Week')
```
  
The chart below shows that there is skewness in our distributions.

```{r Plot - Weekday Summary, message = FALSE, warning = FALSE}
ggplot(weekdaySummary, aes(transactions, fill = dayOfWeek)) + 
  geom_density(alpha = 0.2)
```


### Country Summaries

```{r Plot - country summary, message = FALSE, warning = FALSE}
countrySummary <- customer_data %>%
  group_by(Country) %>%
  summarise(revenue = sum(TotalCost), transactions = n_distinct(InvoiceNo)) %>%
  mutate(aveOrdVal = (round((revenue / transactions),2))) %>%
  ungroup() %>%
  arrange(desc(revenue))
head(countrySummary, n = 10)
```

The chart below shows the top five countries in terms of revenue contribution.

```{r Plot - Top5 Countries Revenue, message = FALSE, warning = FALSE}
#Top five countries in revenue contribution
top5Countries <- customer_data %>%
  filter(Country == 'Netherlands' | Country == 'EIRE' | Country == 'Germany' | Country == 'France' | Country == 'Australia')
```

A table of Top five countries in terms of revenue contribution.

```{r Plot - Top5 Countries Dataframe, message = FALSE, warning = FALSE}
#dataframe of top 5 coutries
top_5 <- top5Countries %>%
  group_by(Country, date) %>%
  dplyr::summarise(revenue = sum(TotalCost), transactions = n_distinct(InvoiceNo), 
                   customers = n_distinct(CustomerID)) %>%
  mutate(aveOrdVal = (round((revenue / transactions),2))) %>%
  ungroup() %>%
  arrange(desc(revenue))
 top_n(top_5, 5)
```

According to the chart below, Netherlands and EIRE have significant sources of revenue. Germany and France also represent significant opportunities but at a lower level.

```{r Plot - Top 5 Countries, message = FALSE, warning = FALSE}
#Plot top 5 country revenue summary
top_5 %>% 
  group_by(Country) %>%
  dplyr::summarise(revenue = sum(revenue)) %>% 
  hchart('treemap', hcaes(x = 'Country', value = 'revenue', color = 'revenue')) %>%
  hc_title(text=" Top 5 Countries by Revenue")
```


## 2.4.2 Customer segmentation


If we can identify a group of customers who make purchases regularly, we can target this group with dedicated marketing campaigns which may reinforce their loyalty. Here we use the CustomerID to look for differences between customers.

We plot the customer summaries from the top 5 countries.

```{r Plot - Customer Summary, message = FALSE, warning = FALSE}
custSummary_1 <- customer_data %>%
  group_by(CustomerID) %>%
  summarise(revenue = sum(TotalCost), transactions = n_distinct(InvoiceNo)) %>%
  mutate(aveOrdVal = (round((revenue / transactions),2))) %>%
  ungroup() %>%
  arrange(desc(revenue))

head(custSummary_1, n = 10)
```

The summary below shows that there seems to be quite a lot of high-quantity sales and refunds.

```{r Summary - Customer Sales & Refunds, message = FALSE, warning = FALSE}
#Seems we have a lot of refunds
#seems there are quite a lot of high-quantity sales and refunds
custSummary_2 <- customer_data %>%
  group_by(CustomerID, InvoiceNo) %>%
  summarise(revenue = sum(TotalCost), transactions = n_distinct(InvoiceNo)) %>%
  mutate(aveOrdVal = (round((revenue / transactions),2))) %>%
  ungroup() %>%
  arrange(revenue) %>%
  mutate(cumsum=cumsum(revenue))

head(custSummary_2, n =10)
```

It seems many of the large transactions are refunded, so if we sum the revenue, we should be working with some reasonable numbers.

```{r Summary - Customer Refunds, message = FALSE, warning = FALSE}
#many large transactions are refunded
#we sum the revenues
custSummary_2 <- customer_data %>%
  group_by(InvoiceNo, CustomerID, Country, date, month, year, hourOfDay, dayOfWeek) %>%
  summarise(orderVal = sum(TotalCost)) %>%
  mutate(recent = Sys.Date() - date) %>%
  ungroup()

custSummary_2$recent <- as.character(custSummary_2$recent)
custSummary_2$recentDays <- sapply(custSummary_2$recent, FUN = function(x) {strsplit(x, split = '[ ]')[[1]][1]})
custSummary_2$recentDays <- as.integer(custSummary_2$recentDays)
head(custSummary_2, n = 5)
```

The dataframe can provide us with the order value and date and time information for each transaction, that can be grouped by customer.

```{r Summary - Customer Info, message = FALSE, warning = FALSE}
customerSummary_3 <- custSummary_2 %>%
  group_by(CustomerID, Country) %>%
  summarise(orders = n_distinct(InvoiceNo), revenue = sum(orderVal), meanRevenue = round(mean(orderVal), 2), medianRevenue = median(orderVal), 
            mostDay = names(which.max(table(dayOfWeek))), mostHour = names(which.max(table(hourOfDay))),
            recency = min(recentDays))%>% #the amount of days that a customer has remained inactive
  ungroup()
head(customerSummary_3)
```

We filter orders greater than 1 and revenue greater than 50 pounds. Our dataframe that gives us a list of repeat customers and tells us their country, how many orders they have made, total revenue and average order value as well as the day of the week and the time of the day they most frequently place orders.

```{r Summary - Customer Final, message = FALSE, warning = FALSE}
customerSummary_3Sum <- customerSummary_3 %>%
  filter(orders > 1, revenue > 50)
head(customerSummary_3Sum)
dim(customerSummary_3Sum) #We remain with a small subset (2,845)
```
We remain with a small subset of 2,845 customers. From this, we are in a good position to answer a number of questions about the customers that we could use for targeted marketing strategies.

```{r Summary - Customers Targetd, message = FALSE, warning = FALSE}
custTargets <- customerSummary_3Sum %>%
  select(recency, revenue, meanRevenue, medianRevenue, orders) %>%
  as.matrix()
rownames(custTargets) <- customerSummary_3Sum$CustomerID
head(custTargets)
```

By analysing how customers cluster, we discover groups of customers that behave in similar ways. This level of customer segmentation is useful in marketing to these groups of customers appropriately. A marketing campaign that works for a group of customers that places low value orders frequently may not be appropriate for customers who place sporadic, high value orders for example.

```{r Plot - Heatmap, message = FALSE, warning = FALSE}
#Generate a heatmap
options(repr.plot.width=20, repr.plot.height=14)
heatmap(scale(custTargets), cexCol = 0.9)
```

Recency refers to the number of days before the reference date when a customer made the last purchase. The lesser the value of recency, higher the likelihood the customer will visit to a store.

Our clustering algorithm aims to keep the distance between data points in a cluster as little as possible relative to the distance between two clusters. Members of separate groups are very distinct whereas individuals of one group are quite similar.

From the heatmap above, it is evident that the total revenue clusters with the number of orders as we would expect. The mean and median order values cluster together, again this is expected, and lastly the order recency sits in its own group. The significant point here is how the customers rows cluster. We are able to uncover groups of customers that behave in similar ways.



# 3. Results

This section presents the prediction approach that was employed, modeling results and discusses the model performance.

### 3.1 The Prediction Approach (Using K-means Algorithm)

*There are 3 main steps in K-Means algorithm (known also as Lloyd’s algorithm):
+ Split samples into initial groups by using seed points. The nearest samples to these seed point will create initial clusters.
+ Calculate samples distances to groups’ central points (centroids) and assign the nearest samples to their cluster.
+ The third step is to calculate newly created (updated) cluster centroids.


### 3.1.2 Using Elbow Method

The main goal behind cluster partitioning methods like k-means is to define the clusters that maintain the intra-cluster variation at a minimum. The plot below denotes the appropriate number of clusters required in our model. In the plot, the location of a bend or a knee is the indication of the optimum number of clusters. 

```{r Plot - Elbow Method, message = FALSE, warning = FALSE}
set.seed(1, sample.kind="Rounding") #using R version 4.0.4`
#We plot optimal number of clusters using factoextra package
fviz_nbclust(custTargets, kmeans, method = "wss") +
  geom_vline(xintercept = 4, linetype = 2)
```

We use the kmeans function to come up with 4 clusters. We then attach the results to CustomersID to identify each customer's cluster.
 
```{r Clusters Kmeans, message = FALSE, warning = FALSE}
clusters <- kmeans(scale(custTargets[,2:4]), 4, nstart = 1) # Performing kmeans with 4 clusters. nstart > 1 is often recommended.
custTargets$Cluster <- as.factor(clusters$cluster) # Attaching the results to CustomersID
custTargetsDf <- as.data.frame(custTargets) # convert matrix to dataframe
```

Based on the Elbow chart produced we conclude that 4 is the optimal number of clusters since it seems to be appearing at the bend in the elbow plot and this is our K value or an optimal number of clusters. Below are the cluster sizes:

```{r Clusters Sizes, message = FALSE, warning = FALSE}
#cluster sizes
clusters$size
```

Below are the cluster means.

```{r Cluster Centers, message = FALSE, warning = FALSE}
#cluster means
clusters$centers
```

+ Cluster 2 represents 4 customers having a high recency and revenue.
+ Cluster 1 consist of 29 customers with high revenue.
+ Cluster 4 comprises of 2,811 customers with high number of orders.

```{r Clusters Orders, message = FALSE, warning = FALSE}
  fviz_cluster(clusters, data=custTargets[, -6], ellipse.type = "norm")
```
# 4. Conclusion 

We managed to identify four segments of customers according to their revenue patterns, product orders and recency which will helps target the customers based on their habits. The clusters created helps us understand the customers in terms of revenue generated, frequency of purchase and days of inactivity. Through Kmeans clustering were able to segment customers to get a better understanding of them which in turn could be used to increase a company's revenue.

Further analysis using other segmentation approaches can be conducted on the clusters to identify more narrowed characteristics of the customers, understand relationship between cluster and types of product purchased or predicting each cluster and customer’s lifetime value.



  


---
title: "HarvardX:PH125.9x Data Science - Capstone CYO: Customer Segmentation Project"
author: "Richard Jonyo"
date: "04 March 2022"

---
